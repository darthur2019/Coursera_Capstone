# -*- coding: utf-8 -*-
# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:percent
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.6.0
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# %% [markdown]
# # Data Science Capstone
#
# This notebook will be used for the IBM Data Science Certification Capstone

# %%
import pandas as pd
import numpy as np
from pandas_profiling import ProfileReport
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
from matplotlib import pyplot

# %%
print("Hello Capstone Project Course")

# %% [markdown]
# ## Introduction: Business Undertanding
#
# The Data for this project was obtained from the City of Seattle's Open Data Portal. The Portal makes data generated by the City openly available to the public for the purpose of increasing the quality of life for the residents, increasing transparency, accountability and comparability, promoting economic development and research, and improving internal performance management.
#
# The data is updated weekly and can be found at the [Seattle Open GeoData Portal](https://opendata.arcgis.com/datasets/5b5c745e0f1f48e7a53acec63a0022ab_0.csv).
#
# The objective of this project is to explore the data, find features important in prediciting the severity of accidents, build models for predicting the severity of an accident and finally selecting the top performing model using various classification metrics with the hope that the insights gathered from this project can help the City of Seattle's Department of Transportation planning and policy making to reduce the number of accidents especially severe accidents.

# %% [markdown]
# ## Data Understanding
#
# At this stage the data is downloaded from the Seattle Open GeoData Portal in the csv format. Features are explored and selected for training the machine learning models and data preprocessing is performed to ensure the data set is robust against biases like unbalanced data, missing values, wrongly input values and also that it is in the right form to aid the best predictive performance. The aim is to obtain good features to reliably predict the severity of an accident
#
# The Data Attributes is listed below for convenience:
#
# |Attribute|Data Type, Length|Description|
# |:-|:-|:-|
# |OBJECTID|ObjectID|ESRI unique identifier|
# |SHAPE|Geometry|ESRI geometry field|
# |INCKEY|Long|A unique key for the incident|
# |COLDETKEY|Long|Secondary key for the incident|
# |ADDRTYPE|Text, 12|Collision address type: Alley, Block, Intersection|
# |INTKEY|Double|Key that corresponds to the intersection associated with a collision|
# |LOCATION|Text, 255|Description of the general location of the collision|
# |EXCEPTRSNCODE|Text, 10|Not specified|
# |EXCEPTRSNDESC|Text, 300|Not specified|
# |SEVERITYCODE|Text, 100|A code that corresponds to the severity of the collision: 3—fatality, 2b—serious injury, 2—injury, 1—prop damage, 0—unknown|
# |SEVERITYDESC|Text|A detailed description of the severity of the collision|
# |COLLISIONTYPE|Text, 300|Collision type|
# |PERSONCOUNT|Double|The total number of people involved in the collision|
# |PEDCOUNT|Double|The number of pedestrians involved in the collision. This is entered by the state.|
# |PEDCYLCOUNT|Double|The number of bicycles involved in the collision. This is entered by the state.|
# |VEHCOUNT|Double|The number of vehicles involved in the collision. This is entered by the state.|
# |INJURIES|Double|The number of total injuries in the collision. This is entered by the state.|
# |SERIOUSINJURIES|Double|The number of serious injuries in the collision. This is entered by the state.|
# |FATALITIES|Double|The number of fatalities in the collision. This is entered by the state.|
# |INCDATE|Date|The date of the incident.|
# |INCDTTM|Text, 30|The date and time of the incident.|
# |JUNCTIONTYPE|Text, 300|Category of junction at which collision took place|
# |SDOT_COLCODE|Text, 10|A code given to the collision by SDOT.|
# |SDOT_COLDESC|Text, 300|A description of the collision corresponding to the collision code.|
# |INATTENTIONIND|Text, 1|Whether or not collision was due to inattention. (Y/N)|
# |UNDERINFL|Text, 10|Whether or not a driver involved was under the influence of drugs or alcohol.|
# |WEATHER|Text, 300|A description of the weather conditions during the time of the collision.|
# |ROADCOND|Text, 300|The condition of the road during the collision.|
# |LIGHTCOND|Text, 300|The light conditions during the collision.|
# |PEDROWNOTGRNT|Text, 1|Whether or not the pedestrian right of way was not granted. (Y/N)|
# |SDOTCOLNUM|Text, 10|A number given to the collision by SDOT.|
# |SPEEDING|Text, 1|Whether or not speeding was a factor in the collision. (Y/N)|
# |ST_COLCODE|Text, 10|A code provided by the state that describes the collision. For more information about these codes, please see the State Collision Code Dictionary.|
# |ST_COLDESC|Text, 300|A description that corresponds to the state’s coding designation.|
# |SEGLANEKEY|Long|A key for the lane segment in which the collision occurred.|
# |CROSSWALKKEY|Long|A key for the crosswalk at which the collision occurred.|
# |HITPARKEDCAR|Text, 1|Whether or not the collision involved hitting a parked car. (Y/N)|

# %%
df = pd.read_csv('C:/Users/david/Downloads/Collisions.csv')

# %%
df.shape

# %% [markdown]
# From initial descriptive statistics it is shown the data set has 221389 observations and 40 features including the
# dependent variable SEVERITYCODE.  
# Preliminary cleanup exercise would be to 
#   1. remove observations with missing values for SEVERITYCODE  
#   2. removing features ending with DESC which are meant to provide text description for the variables ending with CODE
#   3. removing features that have a large number or percentage of missing values
#   4. removing categorical features with high cardinality since using them for model building may introduce high dimensionality in the data set
#   5. removing highly correlated features since they may provide redundant information and may not improve predictive performance
#   

# %%
df1 = df.copy()

# %%
df1.columns.values

# %%
data_clean = df1[['ADDRTYPE','INATTENTIONIND', 'UNDERINFL','WEATHER', 'ROADCOND', 'LIGHTCOND', 'SPEEDING', 'SEVERITYCODE']]
data_clean.info()

# %%
data_clean['SEVERITYCODE'].value_counts().to_frame('count')

# %% [markdown]
# SEVERITYCODE has 5 unique values with its meaning in the table below
#
# | SEVERITYCODE Value | Meaning |
# | :-: | --- |
# | 1 | Accidents resulting in property damage |
# | 2 | Accidents resulting in injuries |
# | 2b | Accidents resulting in serious injuries |
# | 3 | Accidents resulting in fatalities |
# | 0 | Data Unavailable i.e. Blanks |
#
# we can see that observations with value of 0 for SEVERITYCODE will not be meaningful for our modeling

# %%
data_clean.shape

# %%
data_clean['SEVERITYCODE'] = data_clean['SEVERITYCODE'].replace('0', np.nan)

# %%
# simply drop whole row with NaN in "price" column
data_clean.dropna(subset=["SEVERITYCODE"], axis=0, inplace=True)

# reset index, because we droped two rows
data_clean.reset_index(drop=True, inplace=True)

# %%
data_clean.info()

# %%

# %%
data_clean.isna().sum()

# %%
data_final = data_clean[['ADDRTYPE', 'INATTENTIONIND',
       'UNDERINFL', 'WEATHER', 'ROADCOND', 'LIGHTCOND', 'SPEEDING',
       'SEVERITYCODE']]

# %%
data_final.info()

# %%
profile = ProfileReport(data_final, title='Pandas Profiling Report')

# %%
profile.to_widgets()

# %%
data_final['UNDERINFL'] = data_final['UNDERINFL'].map({'N': 0, '0': 0, 'Y': 1, '1': 1})

# %%
data_final['SPEEDING'] = data_final['SPEEDING'].map({'Y': 1})
data_final['SPEEDING'].replace(np.nan, 0, inplace=True)
data_final['SPEEDING'].value_counts().to_frame()

# %%
data_final['INATTENTIONIND'] = data_final['INATTENTIONIND'].map({'Y':1})
data_final['INATTENTIONIND'].replace(np.nan, 0, inplace=True)
data_final['INATTENTIONIND'].value_counts().to_frame()

# %%
data_final['ADDRTYPE'].value_counts().to_frame()

# %%
data_final['INATTENTIONIND'].value_counts().to_frame()

# %%
data_final['UNDERINFL'].value_counts().to_frame()

# %%
data_final['WEATHER'].value_counts().to_frame()

# %%
data_final['WEATHER'] = data_final['WEATHER'].replace('Other', np.nan)
data_final['WEATHER'] = data_final['WEATHER'].replace('Unknown', np.nan)
data_final.dropna(subset=["WEATHER"], axis=0, inplace=True)

# reset index, because we droped two rows
data_final.reset_index(drop=True, inplace=True)

# %%
data_final['ROADCOND'].value_counts().to_frame()

# %%
data_final['ROADCOND'] = data_final['ROADCOND'].replace('Other', np.nan)
data_final['ROADCOND'] = data_final['ROADCOND'].replace('Unknown', np.nan)
data_final.dropna(subset=["ROADCOND"], axis=0, inplace=True)

# reset index, because we droped two rows
data_final.reset_index(drop=True, inplace=True)

# %%
data_final['LIGHTCOND'].value_counts().to_frame()

# %%
data_final['LIGHTCOND'] = data_final['LIGHTCOND'].replace('Other', np.nan)
data_final['LIGHTCOND'] = data_final['LIGHTCOND'].replace('Unknown', np.nan)
data_final.dropna(subset=["LIGHTCOND"], axis=0, inplace=True)

# reset index, because we droped two rows
data_final.reset_index(drop=True, inplace=True)

# %%
data_final = data_final.dropna()
data_final.reset_index(drop=True, inplace=True)

# %%
data_final.info()

# %%
data_final = pd.concat([data_final.drop(['ADDRTYPE','WEATHER', 'ROADCOND', 'LIGHTCOND'], axis=1),
                        pd.get_dummies(data_final['ADDRTYPE']),
                        pd.get_dummies(data_final['WEATHER']),
                        pd.get_dummies(data_final['ROADCOND']),
                        pd.get_dummies(data_final['LIGHTCOND'])], axis=1)

# %%
data_final.info()

# %%
data_final.drop(data_final.columns[data_final.columns.str.contains('Unknown')], axis=1, inplace=True)

# %%
from sklearn import preprocessing
x = data_final.drop(['SEVERITYCODE'], axis=1)
y = data_final['SEVERITYCODE']
data_clean_scaled = preprocessing.StandardScaler().fit(x).transform(x)
data_clean_scaled[0:3]

# %%
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(data_clean_scaled, y, stratify=y,
                                                    test_size=0.33, random_state=12)

# %% [markdown]
#
# # Modeling
#

# %% [markdown]
# We start with a Decision Tree Classifier

# %% [markdown]
# ### Decision Tree Model

# %%
from sklearn.tree import DecisionTreeClassifier

severityTree = DecisionTreeClassifier(criterion="gini", max_depth = 4)
severityTree.fit(x_train,y_train)
predTree = severityTree.predict(x_test)
from sklearn import metrics
print("DecisionTrees's Accuracy: ", metrics.accuracy_score(y_test, predTree))


# %%
print(classification_report(y_test, predTree))

# %%
y.value_counts()

# %%
y_train.value_counts()

# %%
np.unique(predTree)

# %%

# %% [markdown]
# ### Decision Tree Model with class_weight set to balance
#
# This is to accomodate the minority classes that the previous model could not predict because of the severe class imbalance.

# %%
severityTree1 = DecisionTreeClassifier(criterion="gini", max_depth = 4, class_weight='balanced')
severityTree1.fit(x_train,y_train)
predTree1 = severityTree1.predict(x_test)
from sklearn import metrics
print("DecisionTrees's Accuracy: ", metrics.accuracy_score(y_test, predTree1))
print(classification_report(y_test, predTree1))

# %% [markdown]
# ### Random Forest Model

# %%
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV


# %%

param_grid ={'bootstrap': [True, False],
 'max_depth': [5, 10, 20, 30, 40],
 'max_features': ['auto', 'sqrt'],
 'n_estimators': [10, 20, 30]
            }

CV_RF = GridSearchCV(estimator=RandomForestClassifier(class_weight='balanced'), param_grid=param_grid,cv=4)
CV_RF.fit(x_train, y_train)
print('Best Parameters: ', CV_RF.best_params_)

# %%
rf_clf = RandomForestClassifier(bootstrap=False, max_depth=5, max_features='auto', n_estimators=10, random_state=42)
rf_clf.fit(x_train,y_train)
predRF = rf_clf.predict(x_test)
print("DecisionTrees's Accuracy: ", metrics.accuracy_score(y_test, predRF))
print(classification_report(y_test, predRF))

# %% [markdown]
# ### Random Forest Model with class weight set to Balanced

# %%
rf_clf2 = RandomForestClassifier(bootstrap=False, max_depth=5, max_features='auto', n_estimators=10, random_state=42, class_weight='balanced')
rf_clf2.fit(x_train,y_train)
predRF2 = rf_clf2.predict(x_test)
print("RandomForest's Accuracy: ", metrics.accuracy_score(y_test, predRF2))
print(classification_report(y_test, predRF2))

# %%
importance = rf_clf2.feature_importances_
# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
pyplot.bar([x for x in range(len(importance))], importance)
pyplot.show()

# %%
data_final.columns

# %%
data_final.info()

# %%

# %% [markdown]
# ### Logistic Regression Model

# %%
from sklearn.linear_model import LogisticRegression
logRegModel = LogisticRegression(C=0.01)
logRegModel.fit(x_train, y_train)
logRegModel


# %%
predLog = logRegModel.predict(x_test)
print("Logistic Regression Model's Accuracy: ", metrics.accuracy_score(y_test, predLog))
print(classification_report(y_test, predLog))

# %%
logRegModel2 = LogisticRegression(C=0.01, class_weight='balanced')
logRegModel2.fit(x_train, y_train)

# %%
predLog2 = logRegModel2.predict(x_test)
print("Logistic Regression Model2's Accuracy: ", metrics.accuracy_score(y_test, predLog2))
print(classification_report(y_test, predLog2))

# %%
importance = logRegModel2.coef_[0]
# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
pyplot.bar([x for x in range(len(importance))], importance)
pyplot.show()

# %% [markdown]
# ### Support Vector Machine Model

# %%
from sklearn import svm
clf = svm.SVC(kernel='rbf')
clf.fit(x_train, y_train) 

# %%
yhat = clf.predict(X_test)
print (classification_report(y_test, yhat))
print("Support Vector Model's Accuracy: ", metrics.accuracy_score(y_test, yhat))

# %%
